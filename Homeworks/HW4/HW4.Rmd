---
title: "HW4"
author: "POLISCI 251A"
date: "July 28, 2018"
output: html_document
---

**Section: Write the name of your TA here**

**Collaborators: If you worked with someone in this assignment, write their names here**

**Due: Fri Aug 8th at 11:59pm**

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Fake News!

``Who controls the past controls the future. Who controls the present controls the past.``
-- George Orwell, 1984

In this homework we are going to explore how a machine learning algorithm can be used to differentiate two types of news articles. Our dataset includes a combination of reports from two news websites: Politicus USA and The New York Times. The Politicus USA articles were scraped from the ``Featured News`` section of Politicus between January 1 to January 2017. Politicus USA produces articles with extreme left biased stances and, although uses credible media sources such as Reuters, it is considered to publish misleading reports and omit reporting of information that may damage liberal causes, therefore, some reports may be untrustworthy.  For the purposes of this assignment we'll label these articles as ``Fake``. On the other hand, the New York Times is considered to be one of the most reliable news sources in the US (albeit some would consider it having a left-leaning bias). For the purposes of this assignment we'll label these articles as ``Non fake``. 

The underlying question is if ML algorithms could allow us to predict a predetermined label based on linguistic marks. We might expect, for example, that fake news articles would use some words more than others. For example, Kwon et al. (2013) find that non-rumors were more likely to contain positive emotion related words than rumors. Chua and Banerjee (2016) find that verified facts are more associated with the past tense. In sum, there are a lot of people in Silicon Valley and elsewhere trying to figure out how to identify articles with fake news, this means that you're already part of a community working on a relevant topic.

If you want to read more about this issue I recommend the following (real) articles:

- [Fake news for liberals: misinformation starts to lean left under Trump, The Guardian](https://www.theguardian.com/media/2017/feb/06/liberal-fake-news-shift-trump-standing-rock)

- [The Massacre That Wasn't, and a Turning Point for 'Fake News'](https://www.nytimes.com/2017/02/05/business/the-massacre-that-wasnt-and-a-turning-point-for-fake-news.html)

- [Facebook Mounts Effort to Limit Tide of Fake News](https://www.nytimes.com/2016/12/15/technology/facebook-fake-news.html)

And the following papers:

- [Stance Detection for the Fake News Challenge: Identifying Textual Relationships with Deep Neural Nets](https://web.stanford.edu/class/cs224n/reports/2760230.pdf)

- Dale, R. (2017). NLP in a post-truth world. Natural Language Engineering, 23, 319-324. 


# 1. Explore the data (35 pts)

The RData file ``FakeNews`` contains a list called ``challenge`` with two elements:

- term_doc: A dataframe containing the label and the document term matrix.

- articles: A data frame containing the title and complete text of the articles.


## 1.1 Load your data (5pts)

First, load your data and get the elements from the list.

```{r}

```

*What are the dimensions of the document term matrix?*

*How many fake articles has this data?*

```{r}

```


## 1.2. Divide the data (5pts)

Later we'll use the document term matrix (dtm) as a separate object to run our LASSO model, so it is convenient to separate the label and the dtm beforehand. Remember that glmnet accepts only matrices as x.

```{r}

```

## 1.3. Visualize your data (15 pts)

## 1.3.1 Barplots

Let's obtain the 20 most common words for those documents that are classified as fake news vs those classified as non-fake news. Using these common words for each category create a bar plot for each category (*Use the function barplot() and make sure to adjust the arguments such that all the words are readable in the x axis since your grade also depends on the aestetics of your plot*). What do you notice?

```{r}

```

## 1.3.2 Wordclouds

Word clouds are useful tools to visualize text and provide some meaningful interpretation. Using the package ``wordcloud`` create a wordcloud for the 100 most common words in the fake news articles and the non-fake news articles (Set random.order=F). Provide and interpretation for these word clouds. 


```{r}

```


## 1.4. Pre-analysis Qualitative Assesment (10 pts)

Any kind of prediction relies on a good understanding of your data. In this case, this not only implies getting to know which words are used the most but also exploring the articles. It is advisable to read a couple of real and a couple of "alternative" news to understand the nature of the problem. Since articles are quite long it would be difficult to read them in Rstudio,  you can print them in separate text files with a code like this one:

Get one article and it's title:
`article <- cbind(articles[i,2], articles[i,2])``

Print it in a txt file:
`write.table(article, file='article.txt' )`

Of course, you might need to do this in a loop and adjust some parameters for ``write.table()``

Reading some articles could help you to answer questions such as: Should we keep or remove stop words (i.e. words like 'is', 'for', 'the', and other common words)? Should we weight some words more than others? Should we include the length of the article as an explanatory variable? etc.

Using the seed 1984 select 5 articles from the set of fake news and 5 from the set of non-fake news. Print them in a separate csv files, that is, one for real and one for fake (*Note that you will submit these csv file along with your Markdown document*). Read each of the articles carefully. Below, write the code you used to retrieve the articles but only print the title of each article.

```{r, warning=F, message=F}


```

*Provide an overall interpretation of these articles. What differences do yo notice? Do they cover different topics? Do they use different language? Do you think is possible to predict the type article by analyzing it`s language, or are they too similar?*


# 2. LASSO (25 pts)

## 2.1 Logistic regression (5pts)

Using a logistic regression, predict the fake news labels using all the words. What error message do you receive and what do you notice about the coefficients? (*In your final markdown do not print the summary of the results for this regression*)

```{r}

```


## 2.2 Lasso model (5pt)

Using the `glmnet` library, fit a logistic LASSO regression and plot the total number of non-zero coefficients at different values of λ. What do you notice?

```{r, warning=F, message=F}

```

### 2.2.1

We can visualize the coefficients by executing the plot() function and using your model as argument. Provide an interpretation for this plot. 

```{r}

```

### 2.2.2 

Print a summary of the glmnet path at each step along the path using the function print and the model as argument. Here, we have 3 columns:

- df: Degrees of freedom,  the number of nonzero coefficients
- \%Dev: Percentage of the explained deviance
- Lambda: Associated lambda

Take a look to the relationship between these elements and provide an interpretation for each column. 

```{r}

```

## 2.3  Evaluation with arbitrary λ (5pts)

Obtain the λ in the position 50 and using this λ print the non-zero coefficients sorted from  the largest to the smallest. Provide an interpretation for the number of coefficients and their values. 

```{r}

```

## 2.3.1

Using the previous λ get the accuracy of the LASSO model, use a threshold of 0.5. How many fake positives/negatives there are?

```{r}


```

## 2.4 Choosing the right λ (10 pts)

## 2.4.1 Calculate different values of accuracy

Using a for loop calculate the value of λ that provides the highest accuracy (Again, use a thershold of 0.5). Plot the calculated accuracies against the diferent values of λ.


```{r}

```

## 2.4.2 Best λ

What is the λ that maximizes in-sample accuracy?

```{r}

```

What is the value of this accuracy? How much should we trust this value?

```{r}

```


## 2.4.3 Interpretation of the coefficients

Retrieve the non-zero coefficients for this λ. Print them from the largest to the smallest and provide an interpretation. Remember that that these articles were retrieved during electoral campaigns. Try to give a meaninful interpretation of this words given the context. 

```{r}

```


# 3. Crossvalidation (35 pts)

## 3.1 LOOCV (20 pts)

Using a loop perform leave one out cross validation for the LASSO model, storing the level of accuracy for each value of λ. Print the optimized out-of-sample accuracy and its associated λ. How does the out of sample accuracy compare to the in sample accuracy? What about the λ.

[NOTE: The loop might take a while. ]

```{r}
 
```

## 3.1.2

Plot the the estimated accuracy from cross validation (which you can access with `obj$cvm`) vs the λs

```{r}

```


# 3.2 K-fold (10 pts)

Use cv.glmnet to calculate a k-fold cross-validation with nfolds=10

```{r}

```

## 3.2.1 

Plot this object and provide an interpretation 

```{r}

```

## 3.2.2

Retrieve the λ that minimizes the nfold=10 cross-validation 

```{r}

```

Calculate accuracy using this  λ.

```{r}


```

# 3.3 (5 pts)

So far you have obtained 3 different values for λ that optimize accuracy. If you would have to pick one to perform a prediction with a test set which one would you pick and why?


# 4. Improve the model (5pts)

These models were actually tested on a test set of N=578, the accuracy was around 87\% which is not bad, but... we always want to do better! Based on your qualitative assestment of the articles provide some guidelines to get a better out-of sample accuracy. For example, think about if you would remove some words and why, or if you could add some extra features to the document term matrix that might improve out-of-sample accuracy. 




