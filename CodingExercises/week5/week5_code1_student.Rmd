---
title: "Interactive Code Lecture 8 (week 5)"
author: "POLISCI 251A"
date: "July 24, 2018"
output: html_document
---


## 1. Load data and explore

In homework 3 you worked with the credit claiming dataset from the book *The Impression of Influence* by Grimmer, Westwood, and Messing. Using the same dataset, today we are going to work with a data frame containing the 10 most common words. Follow this code to create such matrix:

```{r}
#set up
## Start with a clean environment
rm(list=ls())
## Set your workind directory
setwd("C:/Users/Edgar/Dropbox/PhD/5_year/summer/ML_class/Homework/HW3/")
# load data using the function 'load('fileName')'
load("CreditClaim.RData")
# separate DTM and labels
dtm <- as.data.frame(credit_claim$x)
y <- credit_claim$y
## Create a vector containing the sum of each column 
col_sum_dtm <- colSums(dtm)
## Create a vector called 'common_words' that goes from the most used to the least used word
common_words<- sort(col_sum_dtm, decreasing = T)
# sort the dtm with most common words  -- this makes the next part easier.
dtm_10 <- dtm[,names(common_words)[1:10]]
```

First, run a logistic model using the function ``glm()`` and store your results in an object called ``model_1``. Then, create an object ``predict_glm`` containing the predicted values. Do not forget to include the argument ``type=response``

```{r}
# model 1: Use glm and the argument family=binomial to create a logistic model

## Predicted values 

```

Now, create a function to classify your predictions. The function should take a vector of predicted probabilities and a threshold, and return a vector of classified values (zeroes and ones).

```{r}
################
#  Function to classify predicted probs.
#################

### Use your function to classify the predictions of model 1 with a threshold of 0.5
### Store them in the object "clas_glm"

```

Then, create a function to calculate the error rate. This function should take a vector of labels and a vector of classified values. Since we are working with a qualitative response the error rate should be:
$$\dfrac{1}{n}\sum_{i=1}^{n} I(y_i \neq \hat{y_i}) $$

```{r}
################
#  Function to classify the error rate
#################

## Apply your formula for the object y and the object clas_glm

```

The error rate over the whole sample is around 20\%


## 2. The validation Approach

We explore the use of the validation set approach in order to estimate the test error rates. We begin using the ``sample`` function to split the set of observations into two halves. We refer these observations as the raining set. 

```{r}
### Let's start with a seed of 1
set.seed(1)
train <-  sample( nrow(dtm_10), nrow(dtm_10)/2)
```

We then use the ``subset`` option in ``glm`` to fit a logistic regression using only the observations corresponding to the training set. 

```{r}
#### Fit the model on the training data

### Calcluate the predicting results in the validation set.
### Note that the -train index below selects only the observations that are not in the training set. 

```

Now, apply your classification and error rate function to calculate the error rate of the validation set. 

```{r}
### YOUR CODE HERE

```

Using this splits of observations into a training set and a validation set, we find that the validation set error rates for this model is 18.3\%. If we choose a different training set instead, then we will obtain somewhat different errors on the validation set. Repeat these steps with ``set.seed(2)``.

```{r}
## YOUR CODE HERE

```

Using this splits of observations into a training set and a validation set, we find that the validation set error rates for this model is 22.8\%. This means that our test error estimates are sensitive to the validation set we choose.


## 3. Leave one cross out validation

To perform LOOCV we have to fit a model in some training data, in this case n-1, and make a prediction in the excluded observation. Then, we have to calculate the error an store in a vector. For this reason it is convenient to create an empty vector first. Last week we did this for a simple linear model, in this occasion the code is quite similar but we need to add a step to collect the error rates with our functions. Complete the loop below and calculate the error rate for the LOOCV. 

```{r}
#### Create an empty vector size nrow(dtm_10)
errors_loocv <- rep(NA, nrow(dtm_10))
### Loop over all rows excluding one observation in each interation
for(i in 1:nrow(dtm_10)){
  #### Create a temporary training set by excluding row i
 
  ### Create temporary label for training
 
  ### Create a temporary validation set with only row i
  
  ### Create temporary label for the validation set with only row i
 
  ### Run a logistic model 
  ## Don't forget that your data is train
  
  ## Predict using this model in the validation set
  
  ### Calculate the classification error. 
  ## First, classify
  
  ## Then calculate the eror 

  ## Store in errors_loocv (in position i)
  
}

##### Calculate the mean of the test squared errors (mean of errors_loocv)


```

The LOOCV error rate is $20.45\%$, which is an unbiased estimate of the tests error, since each training contains $n-1 $ observations. 

Alternatively, we can use the ``cv.glm`` function from the ``boot`` library. This function calculates the estimated K-fold cross-validation prediction error for generalized linear models. Let's see how it works

```{r}
### First, install the package and load the library
library(boot)
### To run this function we need to integrate the label and the data
## Do this using the function cbind.data.frame

### And re run your model  1 with this data, call it model_1.1

```

Explore the function ``cv.glm``. Note that the default cost function is the average squared error function which is useful for linear models. However, we have a qualitative response. Let's tweak our error function to the pass it as an argument to the ``cv.glm`` function. 

Create, a function that receives the observed responses and the predicted responses, classifies and then returns the mean test error. In other words, we are integrating our two functions into one. 

```{r}
####Integrated cost function

#### Now, we can implement the cv.glm function. 
## Use these arguments: dtm_10_df, model1, cost and
## store it into an object called cv.err

```

The function returns several values, but we are actually interested in the first value of delta. Retrieve the delta value using ``cv.err$delta[1]`` and compare it with the error rate you calculated in the loop. It should be the same. 

```{r}

```


## 4.  K-Fold validation

The function can be implemented for k-fold validation by including the K argument. Below use k=10, a common choice for k. Notice that the computation is much shorter. What do you notice about the estimated test error rate?

```{r}
## YOUR CODE HERE
## First set a seed

```

Now, use k=5

```{r}
## YOUR CODE HERE

```

**Challenge**

Using a loop calculate the mean test error using K=10 and K=5. Corroborate that your results with the ones obtained above. 
