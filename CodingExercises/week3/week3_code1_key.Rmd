---
title: "Interactive Code Lecture 4"
author: "POLISCI 251A Introduction to Machine Learning"
date: "July 9, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
### Let's start with an clean environment
rm(list=ls())
```

## 1.  Bivariate 


Last class we introduced the time-for-change model, developed by Alan Abramowitz. This model predicts the two-party popular vote based on the state of the economy, the incumbentâ€™s popularity, and the time the incumbent president has been in office.

### 1.1

Download and create in object from the .csv file ``TimeChange.csv" into your computer. 

```{r}
# set your working directory
setwd('C://Users/franc/Dropbox/PhD/5_year/summer/ML_class/Lectures/Lecture3')
# Create an object called 'elect' using the read.csv function
elect<- read.csv('TimeChange.csv')
# View the model
```

### 1.2

Create an object called 'mod' that contains the regression of `IncumbentVoteShare` against `Incumbent_Net_Approval`. The function to create a linear regression works as follows: lm(y~x, data='your_dataset'). Here, 'y' stands for the variable we want to predict (dependent variable) and 'x' for the predictor(s). This function produces the coefficients for the linear model. 

```{r}
mod <- lm(IncumbentVoteShare ~ Incumbent_Net_Approval, data = elect)
# Other approach
#mod <- lm(elect$IncumbentVoteShare ~ elect$Incumbent_Net_Approval)
## Now, use the function summarize to observe the results
summary(mod)
```

### 1.3 

Now, summarize:

1) Fitted Values
2) Residuals

```{r}
## Fitted Values
fit_mod <- mod$fitted.values
# Other approach:
# fit.mod <- predict(mod)
summary(fit_mod)

## Residuals
res_mod <- mod$residuals
# Other approach:
#res.mod <- elect$IncumbentVoteShare - fit_mod
summary(res_mod)

```


### 1.4 Calculate RSS

A common measure of model fit is the Residual Sum of Squares (RSS), also called Sum of Squared Errors (SSE) or Sum of Squared Residuals (SSR):

$RSS= \sum_{i=1}^{N}(Y_i-\hat{Y_i})^2$

Using the objects created above, calculate this measure for our bivariate model. 

```{r}
rss_mod <- sum(res_mod^2)
# Other approach
#rss_mod <- sum((elect$IncumbentVoteShare - fit_mod)^2)
```


### 1.5

Include the fitted values and the residual in your dataset

```{r}
elect$fit_bi <- fit_mod
elect$res_mod <- res_mod
```



*STOP*

## 2. Multivariate 

### 2.1 Regression

Using `lm`, regress `IncumbentVoteShare` on `Q1_GDP_Growth`, `Q2_GDP_Growth`, `Incumbent_Net_Approval`, and `Incumbent_Party_Two_Terms`. Summarize the output from the model.

```{r}
## YOUR CODE HERE
time_for_change <- lm(IncumbentVoteShare ~  Q1_GDP_Growth + Q2_GDP_Growth + Incumbent_Net_Approval +
                 Incumbent_Party_Two_Terms, data=elect)
summary(time_for_change)
```


Using the function 'plot()' and 'elect' as argument create a plot of all elements included in the regression. Be careful to exclude all the variables that are not part of the multivariate model (e.g 'year') **Hint: Use [,]**

```{r}
## YOUR CODE HERE
plot(elect[,c(2:6)])
```



### 2.2 Comparing coefficients

Compare the relationship between `Incumbent_Net_Approval` and `IncumbentVoteShare` in the bivariate and multivariate case.

```{r}
## YOUR CODE HERE
mod$coefficients[2]# .161
time_for_change$coefficients[4] # 0.09511
```

### 2.2 Comparing fit


Compare the RSS of the two models

```{r}
## YOUR CODE HERE
rss_mod 
rss_time <- sum((elect$IncumbentVoteShare - time_for_change$fitted.values)^2)
rss_time
```


### 2.3 Multivariate Prediction

Write a prediction function using the output from the multivariate regression. Note the comment on syntax. 

```{r}
###################
# multi_pred: function to calculate predictions for a 
# multivariate model
# Arguments: 
#  - obj: linear model
#  -indep: vector of values for predictors
#################

## YOUR CODE HERE
multi_pred<- function(obj, indep){
  coefs<- as.numeric(obj$coefficients)
  pred<- coefs[1] + coefs[2] *indep[1] + coefs[3]*indep[2] + coefs[4]*indep[3] + coefs[5]*indep[4]
  return(pred)
}
```

Calculate a prediction for:

- Q1_GDP_Growth: 5.5

- Q2_GDP_Growth: 4.3

- Incumbent_Net_Approval: 3

- Incumbent_Party_Two_Terms : 1

```{r}
## YOUR CODE HERE
multi_pred(time_for_change, c(5.5, 4.3, 3,1))
```

*STOP*

## 3. Test and training sets

Next class, we are going to introduce two fundamental concepts for Machine Learning: 

- Training set

- Test set

In short, we use the training set to fit our model and the test set to asses how well fits the data. 

For larger datasets we normally split our data using a sampling approach. 

Let's start creating two correlated variables as we did last class. 

```{r}
## We need the MASS package
library(MASS)
 
# Let's keep it simple, 
mu <- rep(0,2) ## Mean 0 for both variables
Sigma <- matrix(.7, nrow=2, ncol=2) + diag(2)*.3 ## Correlation matrix

set.seed(124)
rawvars <- mvrnorm(n=100, mu=mu, Sigma=Sigma)
```

Now, let's divide our data set into a training and a test set with 80 percent of the observations in the training set. Follow the steps in this chunk. 

```{r}
## 1. Set your seed
set.seed(124)
## 2.Using the function sample() create a vector size 80 with different row numbers
## Take a look to sample() in the help window
## as x use the number of raws of rawvars
## as size use the 80% of the number of raws of rawvars
## Set replace as false
train_r <-  sample( nrow(rawvars), size=nrow(rawvars)*.8, replace=F)
## 3. Now, using this vector and [,] create and object called train
train <- rawvars[train_r,] 
## 4. Now, using the complement of this vector and [,] create and object called test
## Hint: Use - to retrieve the complement
test <- rawvars[-train_r,]
```



## 4. Extra:  Linear Algebra in R ###

### 4.1

Create three vectors in R.  Make two of the vectors of length 4 and one vector of length 5.
- vector 1 should equal (1, 2, 3, 4)
- vector 2 should equal (3, 4, 5, 6)
- vector 3 should equal (7, 8, 9, 10, 11)

```{r}
test_vec1<- c(1, 2, 3, 4)
test_vec2<- c(3, 4, 5, 6)
test_vec3<- c(7,8, 9, 10, 11)
```

### 4.2

Confirm the length of the vectors using the length function.

```{r}
length(test_vec1)
length(test_vec2)
length(test_vec3)
```

### 4.3

Now, we're going to use `%*%` to take their inner product. First, take the inner product of vector 1 and vector 2. Second, write the inner product of vector 1 and vector 3. What do you notice? 

```{r}
# test_vec1%*%test_vec2
# test_vec1%*%test_vec3
```

## 5. Vector based regression

### 5.1 

Write a function to predict from a multivariate regression using vectors.

```{r}
multi_pred_vec<- function(obj, indep){
  coefs<- obj$coefficients
  indep.mat <- as.matrix(cbind(1, indep))
  pred<- c(indep.mat%*%coefs)
  return(pred)
}
```

### 5.2

Compare the predicted values between your two predict functions (one with linear algebra, one without) and from the lm ("canned") model.

```{r}
time_for_change$fitted.values
elect.ind.vars <- elect[c(2:5)]
multi_pred_vec(time_for_change, elect.ind.vars)
```








