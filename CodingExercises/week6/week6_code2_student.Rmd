---
title: "Interactive Code Lecture 10 (week 6)"
author: "POLISCI 251A"
date: "July 31, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## 1. Setting up New York Times Annotated Corpus

### 1.1

Today, we are going to analyze again the New York Times Annotated Corpus. This corpus comes from the NYT API and was originally in json format. Using Python the file was transformed into a Document Term Matrix usable for analysis. 

From the coursework site please download `NYT.RData` and load the file.

```{r}
rm(list=ls())
setwd('C:/Users/franc/Dropbox/PhD/5_year/summer/ML_class/Lectures/Lecture9/')
load("NYT.RData")
```

This loads a list, `nyt_list`, with the following components:
- train : the document term matrix for the training set
- train_label: an indicator equal to 1 if the story comes from the national desk for each document in the training set
- test: the document term matrix for the test set.  
- test_label: an indicator equal to 1 if the story comes from the national desk for each document in the test set

We will work with `train` and `train_label` to build our prediction models. We will use the `test` set to test the fit of our model.  

Let's put these components in individual objects. To retrieve elements from a list we can use double square brackets: ``[[]]``

```{r}
## Training datset and label
train<- nyt_list[[1]]    
train_label<- nyt_list[[2]]
## Test dataset and label
test<- nyt_list[[3]]
test_label<- nyt_list[[4]]
```


Fit a LASSO linear regression with the training set (we did this last class.)

```{r}

```

## 1.2 Plot

We can visualize the coefficients by executing the plot() function and using your model as argument.
Default: plots against the L1-norm of the coefficients

```{r}

```

If we set the argument xvar as 'lambda' we can visualize the log lambda in the x axis. 

```{r}

```

## 1.3 Print elements

Print a summary of the glmnet path at each step along the path using the function print and the model as argument. Here, we have 3 columns:

- df: Degrees of freedom,  the number of nonzero coefficients
- \%Dev: Percentage of the explained deviance
- Lambda: Associated lambda

Take a look to the relationship between these elements. 

```{r}
### Use the function print() and your model as argument

```

## 1.4 Print coefficients

Let's pick an arbitrary lambda, let's say lambda=0.023530. We are going to take look to the non-zero coefficients associated with this lambda. 

```{r}
# First, create an object with the function coef(). Use your model as first argument and s=0.023530

# Let's print the non-zero coefficients
## Fron this object, select only those that are non zero

## Sort them from large to small

##Print the length of this vector

## Print the result


```

There are 77 non-zero coefficients, the positive coefficients are more associated with the national desk, whereas the negative coefficients are less related with this desk. 

## 2. Finding Lambda

### 2.1

We're now ready to devise a method for selecting the appropriate value of lambda. First we need to define our loss function. To do this, write a function calculate the mean squared error:

```{r}
## Write a function that receives predictions and observed values
## and calculates the mean squaerd errors



```

### 2.2

Now, let's calculate the MSE for the in-sample fit from the LASSO regression. 

1. Make in-sample predictions using LASSO for each value of lambda. 

2. Then calculate the MSE across those predictions. 

3. Finally, make a plot of the MSE values against lambda values.

```{r}
# make predictions for each value of lamda using the train data

## Take a look to this object

# calculate MSE for each lamda value


# plot MSE x lamda

```

Recalling that smaller MSE is better, what does the in sample fit tell us is the optimal lambda value? How much are we penalizing the model then? 

*Using in-sample fit we can see that the penalty is relatively small, that is, we are still including a high number of coefficients*


Retrieve the lambda that minimizes the mse:

```{r}

```

Now use this lambda to observe the coefficients that the model is keeping. 

```{r}
# First, create an object with the function coef(). Use your model as first argument and your lambda that minimizes the mse

# Let's print the non-zero coefficients
## Fron this object, select only those that are non zero

## Sort them from large to small

##Print the length of this vector

## Print the result

```

# 3. Cross Validation

### 3.1 

We want to devise a way to do cross validation. With LASSO we will have a canned method for doing the cross validation. 

(I provide instructions below on how to manually perform cross validation---helpful for applying the procedure to many other methods.)

To perform cross validation with glmnet we use cv.glmnet

```{r}
### Create a cv.glmnet object

```

You can specify the loss function (with `type.measure =` ). For example, for classification you might pick accuracy.  The default is MSE. `nfolds` allows you to set the number of folds used for cross validation.

There are lots of built in functions in cv.glmnet. Try plotting the cv.glmnet object you created. This shows how the Mean-Squared Error for the cross validated predictions changes across different values of lambda.

```{r}

```

We can access the lambda values  `obj$lambda`. 

```{r}

```

and the lambdas  that lead to the smallest mse with `obj$lambda.min`

```{r}

```


### 3.2 

The plot object gave us the in sample fit.  Let's see how it compares to the out-of-sample fit.

To do this, make predictions for the test data using each value of lambda from cv.glmnet (lasso_cv$lambda)

```{r}
## Create an empty vector to store your results

## Loop over the lambdas from cross validation. 
## In each step predict using the test matrix and calculate the mse

```


Retrieve the lambda that minimizes out-of-sample mse:

```{r}

```


### 3.3

Plot the the estimated mse from cross validation (which you can access with `obj$cvm`) vs the lambdas


```{r}

```

Plot the out-of-sample mse against the estimated mse from cross validation (which you can access with `obj$cvm`).

```{r}

```


How well did cross validation do in selecting the appropriate value of lambda?


## Bonus section: your own cross validation 

I fit my own cross validations using the following procedure:

1) Use the sample function with replace = T to assign each observation to a fold
2) Use a for loop to train on the K-1 subsets of the data
3) Predict for the held out data for that round
4) I then store those out of sample predictions

Using this procedure, manually determine how many of the top occurring words you should include to make the best mean squared error predictions
